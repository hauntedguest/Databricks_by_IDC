# üöÄ Databricks 14 Days AI Challenge

This repository documents my **14-day hands-on learning journey on Databricks**, focused on building a strong foundation in **data engineering, analytics, and AI workflows** using Apache Spark and the Lakehouse architecture.

The challenge follows a **daily structured format** combining concepts, practice tasks, and real-world use cases.

## üìå Table of Contents

- [About the Challenge](#-about-the-challenge)
- [What You‚Äôll Learn](#-what-youll-learn)
- [Challenge Structure](#-challenge-structure)
- [Daily Progress Tracker](#-daily-progress-tracker)
- [Tech Stack](#-tech-stack)
- [Learning Objective](#-learning-objective)
- [Acknowledgements](#-acknowledgements)
- [Connect With Me](#-connect-with-me)
- [Challenge Snapshot](#-challenge-snapshot)
  
---

## üè¢ About the Challenge

- üìå Organized by **Indian Data Club**
- ü§ù In collaboration with **Codebasics**
- ‚≠ê Sponsored by **Databricks**

This initiative is designed to help learners move from **Databricks fundamentals to AI-powered analytics** through practical, project-based learning.

---

## üéØ What You‚Äôll Learn

By completing this 14-day journey, learners will gain a solid foundation in **data engineering, analytics, and AI on Databricks**. Key learning areas include:  

- **Databricks Platform & Lakehouse Architecture** ‚Äì Understanding the ecosystem and how data flows from raw to analytical layers.  
- **Apache Spark & PySpark** ‚Äì Performing distributed data processing and transformations on large datasets.  
- **Delta Lake & Data Engineering Patterns** ‚Äì Implementing ACID transactions, versioning, and structured pipelines.  
- **Medallion Architecture (Bronze‚ÄìSilver‚ÄìGold)** ‚Äì Building clean and efficient layers for staged and curated datasets.  
- **Workflows, Jobs & Governance** ‚Äì Automating pipelines and managing access with Unity Catalog.  
- **SQL Analytics & Performance Optimization** ‚Äì Running queries efficiently and tuning performance for analytics workloads.  
- **MLflow, AI & Generative AI** ‚Äì Managing ML experiments, comparing models, and experimenting with AI-driven insights.  

This ensures a **comprehensive understanding of the Databricks ecosystem**, bridging the gap between theory and real-world application.  

---

## üó∫Ô∏è Challenge Structure

The challenge is structured into **four phases**, progressing from fundamentals to advanced AI analytics:  

### **PHASE 1: Foundation (Days 1‚Äì4)**  
Covers Databricks setup, basic concepts, Spark fundamentals, and data transformations. Learners gain hands-on experience with notebooks, RDDs, and DataFrames.  

### **PHASE 2: Data Engineering (Days 5‚Äì8)**  
Focuses on **Delta Lake, Medallion Architecture**, and job automation. Students learn to design efficient pipelines, implement versioning, and manage governance.  

### **PHASE 3: Advanced Analytics (Days 9‚Äì11)**  
Teaches **SQL analytics, performance tuning, and statistical preparation**. Learners work on real datasets to create interactive queries and optimized analytics.  

### **PHASE 4: AI & ML (Days 12‚Äì14)**  
Introduces **MLflow, model comparison, and generative AI techniques**. Learners track experiments, evaluate models, and build AI-driven solutions to extract insights.  

By the end of the challenge, learners can confidently handle **end-to-end Databricks workflows**, from raw data ingestion to AI-powered analytics.

---

## üß† Daily Progress Tracker

| Day | Topic | Description | LinkedIn Post |
|----|------|-------------|---------------|
| ‚úÖ D0 | Setup & Data Loading (Prerequisites) | Installed Databricks and prepared datasets for the challenge | [LinkedIn Post](******) |
| ‚úÖ D1 | Platform Setup & First Steps üöÄ | Explored Databricks workspace, notebooks, and first basic operations | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_sqlwithidc-databricks-datasciencejourney-activity-7415453241540284416-Vn1n?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D2 | Apache Spark Fundamentals ‚ö°| Learned Spark RDDs, DataFrames, and basic transformations | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_sqlwithidc-databricks-apachespark-activity-7415820639049056256-TVlU?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D3 | PySpark Transformations | Applied PySpark operations to clean and transform datasets efficiently | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_sqlwithidc-databricks-pyspark-activity-7416113048500883457-5oKi?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D4 | Delta Lake Basics | Implemented Delta Lake tables and learned ACID transactions | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_sqlwithidc-databricks-deltalake-activity-7416545047757750272-DTLi?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D5 | Delta Lake Advanced | Explored versioning, time travel, and advanced Delta Lake features | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_sqlwithidc-databricks-deltalake-activity-7417003885111857152-1_06?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D6 | Medallion Architecture | Designed Bronze‚ÄìSilver‚ÄìGold layers for structured data pipelines | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_sqlwithidc-databricks-medallionarchitecture-activity-7417270498893520899-qnft?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D7 | Workflows & Jobs | Scheduled and automated Databricks jobs for data pipelines | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_sqlwithidc-databricks-dataengineering-activity-7417653747771813889-39wY?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D8 | Unity Catalog | Managed data governance, permissions, and catalog in Databricks | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_databricks-unitycatalog-dataengineering-activity-7417997615671422976-Q6pK?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D9 | SQL Analytics | Ran SQL queries on Databricks and created interactive reports | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_databricks-sqlanalytics-dataengineering-activity-7418345230783094784-kQCc?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D10 | Performance Optimization | Applied caching, partitioning, and optimization techniques | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_databrickswithidc-sqlwithidc-performanceoptimization-activity-7419042536515727361-86C1?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D11 | Stats & ML Prep | Performed statistical analysis and prepared datasets for ML | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_databrickswithidc-sqlwithidc-statistics-activity-7419050133411049472-IDuJ?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| ‚úÖ D12 | MLflow Basics | Tracked machine learning experiments and models using MLflow | [LinkedIn Post](https://www.linkedin.com/posts/pranav-gupta-264438228_databrickswithidc-mlflow-machinelearning-activity-7419461160644657152-zh_N?utm_source=share&utm_medium=member_desktop&rcm=ACoAADkEWmwBofIhY5MOdN9CcXMsf4mMad2Mbbk) |
| üîÑ D13 | Model Comparison | Compared model performances and evaluated metrics | [LinkedIn Post](******) |
| üîÑ D14 | AI & Generative Analytics | Built AI pipelines and experimented with generative AI models | [LinkedIn Post](******) |

---

## üõ†Ô∏è Tech Stack

For this 14-day challenge, I primarily used the **Databricks Community Edition**, which offers a free and powerful environment for learning **big data processing** and **AI workflows**.  
The main tools and frameworks included:  

- **Apache Spark & PySpark** ‚Äì For distributed data processing, transformations, and analytics at scale.  
- **Delta Lake** ‚Äì To implement reliable, ACID-compliant data lakes with version control and time travel.  
- **SQL** ‚Äì For querying structured datasets and running analytical operations efficiently.  
- **MLflow** ‚Äì To track machine learning experiments, register models, and manage workflows.  
- **Python** ‚Äì As the primary programming language for data manipulation, analytics, and AI tasks.  

This stack allowed me to explore the full **data-to-AI pipeline** in a hands-on environment. 

---

## üå± Learning Objective

The objective of this challenge was to develop **industry-ready skills** in Databricks and big data analytics.  
By following a structured approach, I focused on:  

- Gaining **concept clarity** in data engineering and AI workflows  
- Practicing hands-on tasks with **real-world datasets**  
- Understanding **data governance, optimization, and performance tuning**  
- Building **AI-driven analytics** solutions with MLflow and generative AI  

The goal was to combine theoretical knowledge with practical application to become confident in **end-to-end Databricks projects**.

---

## ü§ù Acknowledgements

Thanks to **Indian Data Club**, **Codebasics**, and **Databricks** for designing this practical and beginner-friendly AI challenge.

---

## üì¨ Connect With Me

<!-- Typing Animation / ü§ù Connect with me -->
[![Typing SVG](https://readme-typing-svg.herokuapp.com?color=0DAD8D&lines=Let‚Äôs+connect+and+collaborate+on+meaningful+projects!;Click+the+buttons+below+to+connect+with+me+directly!)](https://git.io/typing-svg)

<div align="center">
<!-- üíº LinkedIn -->
<a href="https://www.linkedin.com/in/pranav-gupta-264438228/"><img src="https://cdn-icons-png.flaticon.com/512/174/174857.png" alt="LinkedIn" width="30" height="30"/></a>
<!-- üìÆ Gmail -->
<a href="mailto:pranavgupta7995@gmail.com" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/732/732200.png" alt="Email" width="35" height="35"></a> 
<!-- üÜî GitHub -->
<a href="https://github.com/hauntedguest" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/733/733553.png" alt="GitHub" width="35" height="35"></a>


<!-- Typing Animation / ü§ù Thanks for Visiting! -->
[![Typing SVG](https://readme-typing-svg.herokuapp.com?color=8A2BE2&lines=ü§ùThank+you+for+visiting+my+profile!)](https://git.io/typing-svg)

<!-- ‚≠êüí´ Shower stars if you like my repos -->
<div align="center">
<img src="https://media.giphy.com/media/ObNTw8Uzwy6KQ/giphy.gif" width="30">
<a href="https://github.com/hauntedguest" alt="GitHub Stars" title="Star my repositories">
<img src="https://img.shields.io/badge/Shower_stars_if_you_like_my_repositories-15k?style=for-the-badge&color=f9c513&logo=github&logoColor=black"/>
</a>
</div>

---

## üñºÔ∏è Challenge Snapshot

![Challenge Snapshot](Databricks%20AI.png)

---
